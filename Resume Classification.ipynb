{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QuxYf07d62Oo"
      },
      "source": [
        "Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates. Finding suitable candidates for an open role from a database of 1000s of resumes can be a tough task. Automated resume categorization can speeden the candidate selection process. Such automation can really ease the tedious process of fair screening and shortlisting the right candidates and aid quick decisionmaking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t0FQjPs8o4s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "standing-zimbabwe"
      },
      "source": [
        "#### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "universal-jonathan",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@title Download the data\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/UpdatedResumeDataSet.csv"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T3FkLI6wcaat"
      },
      "source": [
        "Read the UpdatedResume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIu-AOsB9GfD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('UpdatedResumeDataSet.csv', encoding='utf-8')\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QvddL7X69NiB"
      },
      "source": [
        "### Pre-processing and EDA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1d25HrzTGW"
      },
      "source": [
        "Display  all the categories of resumes and their counts in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C92ji6ZV9MWs"
      },
      "outputs": [],
      "source": [
        "# Displaying the distinct categories of resume\n",
        "print(df['Category'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtIjY7ji9va5"
      },
      "outputs": [],
      "source": [
        "# Displaying the distinct categories of resume and the number of records belonging to each category\n",
        "print(df['Category'].value_counts())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHv50ojzvO5"
      },
      "source": [
        "Create the count plot of different categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYwrK_5f93gP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.xticks(rotation=90)\n",
        "sns.countplot(y=\"Category\", data=df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VyXtz0Mr0NVB"
      },
      "source": [
        "Create a pie plot depicting the percentage of resume distributions category-wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrpJDoGx-CuF"
      },
      "outputs": [],
      "source": [
        "targetCounts = df['Category'].value_counts()\n",
        "targetLabels  = targetCounts.index\n",
        "# Make square figures and axes\n",
        "plt.figure(1, figsize=(25,25))\n",
        "the_grid = GridSpec(2, 2)\n",
        "\n",
        "\n",
        "cmap = plt.get_cmap('coolwarm')\n",
        "colors = [cmap(i) for i in np.linspace(0, 1)]\n",
        "plt.subplot(the_grid[0, 1], aspect=1, title='RESUME CATEGORY DISTRIBUTION')\n",
        "\n",
        "source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True, colors=colors)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaznteJ1xr4"
      },
      "source": [
        "Convert all the `Resume` text to lower case \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf1wNRqb-Om2"
      },
      "outputs": [],
      "source": [
        "# Convert all characters to lowercase\n",
        "df['Resume']=df['Resume'].str.lower()\n",
        "print(df['Resume'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBXmdXpDIQJ"
      },
      "source": [
        "Cleaning Resume"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yvkRbRnM3ap7"
      },
      "source": [
        "Define a function to clean the resume text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Z-Oois_LWE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def cleanResume(resumeText):\n",
        "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
        "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
        "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
        "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
        "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
        "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText)\n",
        "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
        "    return resumeText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O2tR_IjDNWr"
      },
      "outputs": [],
      "source": [
        "df['cleaned_resume'] = df['Resume'].apply(lambda x: cleanResume(x))\n",
        "print(df['cleaned_resume'][31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXb37CNsIv1x"
      },
      "outputs": [],
      "source": [
        "df.head()    # data after cleaning the resume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EnSuTubI_S1"
      },
      "outputs": [],
      "source": [
        "sent_lens = []\n",
        "for i in df.cleaned_resume:\n",
        "    length = len(i.split())\n",
        "    sent_lens.append(length)\n",
        "\n",
        "print(len(sent_lens))\n",
        "print(max(sent_lens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7BXiN6NJMc6"
      },
      "outputs": [],
      "source": [
        "df[\"Resume\"][100] ,  df[\"cleaned_resume\"][100]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ktaCkU5yG5V0"
      },
      "source": [
        "### Stop Words Removal"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HWP5cZL5_f0V"
      },
      "source": [
        "Use `nltk` package to find the most common words from the `cleaned resume` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvSiTsqqXjeT"
      },
      "outputs": [],
      "source": [
        "# stop words\n",
        "oneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\n",
        "oneSetOfStopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v9bZeMwX0lN"
      },
      "outputs": [],
      "source": [
        "# most common words\n",
        "totalWords =[]\n",
        "Sentences = df['cleaned_resume'].values\n",
        "cleanedSentences = \"\"\n",
        "for i in range(0,160):\n",
        "    cleanedText = cleanResume(Sentences[i])\n",
        "    cleanedSentences += cleanedText\n",
        "    requiredWords = nltk.word_tokenize(cleanedText)\n",
        "    for word in requiredWords:\n",
        "        if word not in oneSetOfStopWords and word not in string.punctuation:\n",
        "            totalWords.append(word)\n",
        "\n",
        "wordfreqdist = nltk.FreqDist(totalWords)\n",
        "mostcommon = wordfreqdist.most_common(50)\n",
        "print(mostcommon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYAJU8teYBlv"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud().generate(cleanedSentences)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2pBLoWMxAf64"
      },
      "source": [
        "Convert the categorical variable `Category` to a numerical feature and make a different column, which can be treated as the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg4QNr7DYSJ5"
      },
      "outputs": [],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "df[\"Category_Labelled\"] = labelencoder.fit_transform(df[\"Category\"])\n",
        "# print(type(labels))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HUauwAsOn31f"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bTTiMQvYA77m"
      },
      "source": [
        "Convert the text to feature vectors by applying `tfidf vectorizer` to the Label encoded category made above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUsZIUQyY_wX"
      },
      "outputs": [],
      "source": [
        "Text = df['cleaned_resume'].values\n",
        "op_labels = df['Category_Labelled'].values\n",
        "word_vectorizer = TfidfVectorizer(max_features = 1500)\n",
        "word_vectorizer.fit(Text)\n",
        "features = word_vectorizer.transform(Text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07MC_nseoJyH"
      },
      "outputs": [],
      "source": [
        "word_vectorizer.get_feature_names()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPTb3I-Oon2D"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocjoqFlCFJn3"
      },
      "source": [
        "Split the data into train and test sets. Apply Naive Bayes Classifier (MultinomialNB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYoRwtlWobMI"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(features,op_labels,random_state=0, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3PSgkj8om8l"
      },
      "outputs": [],
      "source": [
        "clf = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rwZ6R6Iokfo"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3qafOcv6LTl"
      },
      "outputs": [],
      "source": [
        "print('Accuracy of NaiveBayes Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEZcoy36Tve"
      },
      "outputs": [],
      "source": [
        "pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koCNI4f-6cyf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Classification report for classifier %s:\\n%s\\n\" % (clf, metrics.classification_report(y_test, pred)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
